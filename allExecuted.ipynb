{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sunny' 'Warm' 'Normal' 'Strong' 'Warm' 'Same']\n",
      " ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      " ['Rainy' 'Cold' 'High' 'Strong' 'Warm' 'Change']\n",
      " ['Sunny' 'Warm' 'High' 'Strong' 'Cool' 'Change']]\n",
      "['Yes' 'Yes' 'No' 'Yes']\n",
      "['Sunny' 'Warm' '?' 'Strong' '?' '?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data=pd.read_csv('data1.csv'))\n",
    "\n",
    "concepts = np.array(data.iloc[:,0:-1])\n",
    "print(concepts)\n",
    "\n",
    "target = np.array(data.iloc[:,-1])\n",
    "print(target)\n",
    "\n",
    "def learn(concepts, target):\n",
    "    specific_h = concepts[0].copy()\n",
    "\n",
    "    for i, h in enumerate(concepts):\n",
    "        if target[i] == \"Yes\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "\n",
    "                    specific_h[x] = \"?\"\n",
    "    return specific_h\n",
    "\n",
    "specific_h = learn(concepts, target)\n",
    "print(specific_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dataset is: \n",
      "\n",
      "     Sky Airtemp Humidity    Wind Water Forecast EnjoySport\n",
      "0  Sunny    Warm   Normal  Strong  Warm     Same        Yes\n",
      "1  Sunny    Warm     High  Strong  Warm     Same        Yes\n",
      "2  Rainy    Cold     High  Strong  Warm   Change         No\n",
      "3  Sunny    Warm     High  Strong  Cool   Change        Yes\n",
      "\n",
      " The Concepts are: \n",
      " [['Sunny' 'Warm' 'Normal' 'Strong' 'Warm' 'Same']\n",
      " ['Sunny' 'Warm' 'High' 'Strong' 'Warm' 'Same']\n",
      " ['Rainy' 'Cold' 'High' 'Strong' 'Warm' 'Change']\n",
      " ['Sunny' 'Warm' 'High' 'Strong' 'Cool' 'Change']]\n",
      "\n",
      "The target is: \n",
      " ['Yes' 'Yes' 'No' 'Yes']\n",
      "\n",
      "\n",
      "Final S: ['Sunny' 'Warm' '?' 'Strong' '?' '?']\n",
      "\n",
      "\n",
      "Final G: [['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(data=pd.read_csv('data2.csv'))\n",
    "print('The Dataset is: \\n')\n",
    "print(data)\n",
    "\n",
    "concepts = np.array(data.iloc[:,0:-1])\n",
    "\n",
    "print('\\n The Concepts are: \\n',concepts)\n",
    "target = np.array(data.iloc[:,-1])\n",
    "print('\\nThe target is: \\n',target)\n",
    "\n",
    "def learn(concepts, target):\n",
    "    specific_h = concepts[0].copy()\n",
    "    general_h = [[\"?\" for i in range(len(specific_h))] for i in range(len(specific_h))]\n",
    "    for i, h in enumerate(concepts):\n",
    "        if target[i] == \"Yes\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    specific_h[x] = '?'\n",
    "                    general_h[x][x] = '?'\n",
    "\n",
    "\n",
    "        if target[i] == \"No\":\n",
    "            for x in range(len(specific_h)):\n",
    "                if h[x] != specific_h[x]:\n",
    "                    general_h[x][x] = specific_h[x]\n",
    "                else:\n",
    "                    general_h[x][x] = '?'\n",
    "\n",
    "    indices = [i for i,val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]\n",
    "\n",
    "    for i in indices:\n",
    "        general_h.remove(['?', '?', '?', '?', '?', '?'])\n",
    "\n",
    "    return specific_h, general_h\n",
    "\n",
    "s_final, g_final = learn(concepts, target)\n",
    "print(\"\\n\\nFinal S:\", s_final)\n",
    "print(\"\\n\\nFinal G:\", g_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook Temperature Humidity    Wind class\n",
      "0      Sunny         Hot     High    Weak    No\n",
      "1      Sunny         Hot     High  Strong    No\n",
      "2   Overcast         Hot     High    Weak   Yes\n",
      "3       Rain        Mild     High    Weak   Yes\n",
      "4       Rain        Cool   Normal    Weak   Yes\n",
      "5       Rain        Cool   Normal  Strong    No\n",
      "6   Overcast        Cool   Normal  Strong   Yes\n",
      "7      Sunny        Mild     High    Weak    No\n",
      "8      Sunny        Cool   Normal    Weak   Yes\n",
      "9       Rain        Mild   Normal    Weak   Yes\n",
      "10     Sunny        Mild   Normal  Strong   Yes\n",
      "11  Overcast        Mild     High  Strong   Yes\n",
      "12  Overcast         Hot   Normal    Weak   Yes\n",
      "{'Outlook': {'Overcast': 'Yes',\n",
      "             'Rain': {'Wind': {'Strong': 'No', 'Weak': 'Yes'}},\n",
      "             'Sunny': {'Humidity': {'High': 'No', 'Normal': 'Yes'}}}}\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "dataset = pd.read_csv('data3.csv')\n",
    "features=['Outlook','Temperature','Humidity','Wind']\n",
    "def entropy(target_col):\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data,split_attribute_name,target_name=\"EnjoySport\"):\n",
    "     total_entropy = entropy(data[target_name])\n",
    "     vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "     Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "     Information_Gain = total_entropy - Weighted_Entropy\n",
    "     return Information_Gain\n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n",
    "\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "\n",
    "    else:\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features]\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        tree = {best_feature:{}}\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            tree[best_feature][value] = subtree\n",
    "\n",
    "        return(tree)\n",
    "\n",
    "def predict(query,tree,default = 1):\n",
    "\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]]\n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "training_data = dataset.iloc[:13]\n",
    "print(training_data)\n",
    "\n",
    "features=['Outlook','Temperature','Humidity','Wind']\n",
    "target_attribute_name=\"class\"\n",
    "parent_node_class=None\n",
    "tree=ID3(training_data,training_data,features,target_attribute_name,parent_node_class)\n",
    "pprint(tree)\n",
    "\n",
    "query=dataset.iloc[:,:-1].to_dict(orient=\"records\")\n",
    "result=predict(query[10],tree,1.0)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      "[[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Learned Output: \n",
      " [[0.89481338]\n",
      " [0.8771683 ]\n",
      " [0.89751339]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
    "y = np.array(([92], [86], [89]), dtype=float)\n",
    "X = X/np.amax(X,axis=0)\n",
    "y = y/100\n",
    "def sigmoid (x):\n",
    "\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def derivatives_sigmoid(x):\n",
    "\n",
    "    return x * (1 - x)\n",
    "\n",
    "epoch=7000 #Setting training iterations\n",
    "\n",
    "lr=0.1 #Setting learning rate\n",
    "inputlayer_neurons = 2 #number of features in data set\n",
    "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
    "output_neurons = 1 #number of neurons at output layer\n",
    "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "bout=np.random.uniform(size=(1,output_neurons))\n",
    "for i in range(epoch):\n",
    "    h_ip=np.dot(X,wh) + bh\n",
    "    h_act = sigmoid(h_ip)\n",
    "    o_ip=np.dot(h_act,wout) + bout\n",
    "    output = sigmoid(o_ip)\n",
    "    EO = y-output\n",
    "    outgrad = derivatives_sigmoid(output)\n",
    "    d_output = EO* outgrad\n",
    "    Eh = d_output.dot(wout.T)\n",
    "    hiddengrad = derivatives_sigmoid(h_act)\n",
    "    d_hidden = Eh * hiddengrad\n",
    "    wout += h_act.T.dot(d_output) *lr\n",
    "    wh += X.T.dot(d_hidden) *lr\n",
    "\n",
    "print(\"Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Learned Output: \\n\" ,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t Naive Bayesian Classifier\n",
      "\n",
      "The  1 Test data is:\n",
      " Outlook  =  Sunny ,  Temperature  =  Mild ,  Humidity  =  Normal ,  Wind  =  Strong\n",
      "Probability of Yes:  0.014109347442680773 \n",
      "Probability of No:  0.010285714285714285\n",
      "Classified as YES\n",
      "\n",
      "\n",
      "The  2 Test data is:\n",
      " Outlook  =  Overcast ,  Temperature  =  Mild ,  Humidity  =  High ,  Wind  =  Strong\n",
      "Probability of Yes:  0.014109347442680773 \n",
      "Probability of No:  0.0\n",
      "Classified as YES\n",
      "\n",
      "\n",
      "The  3 Test data is:\n",
      " Outlook  =  Overcast ,  Temperature  =  Hot ,  Humidity  =  Normal ,  Wind  =  Weak\n",
      "Probability of Yes:  0.028218694885361547 \n",
      "Probability of No:  0.0\n",
      "Classified as YES\n",
      "\n",
      "\n",
      "The  4 Test data is:\n",
      " Outlook  =  Rain ,  Temperature  =  Mild ,  Humidity  =  High ,  Wind  =  Strong\n",
      "Probability of Yes:  0.010582010582010581 \n",
      "Probability of No:  0.027428571428571438\n",
      "Classified as NO\n",
      "\n",
      "\n",
      "Accuracy of the Classifier is:  1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#reading the CSV file\n",
    "data1 = pd.read_csv('data5.csv')\n",
    "\n",
    "#Separating all Yes in one Dataframe\n",
    "df1 = data1[data1['class'] == 'Yes']\n",
    "\n",
    "#Separating all No in one Dataframe\n",
    "df2 = data1[data1['class'] == 'No']\n",
    "\n",
    "#Declaring list variable for storing the input from the user\n",
    "inputlist1=[]\n",
    "\n",
    "#Initializing the Header value in head list\n",
    "head=['Outlook','Temperature','Humidity','Wind','class']\n",
    "inputlist1.append(head)\n",
    "count=0\n",
    "\n",
    "def counting(inputlist1,j,str1,count):\n",
    "    if(inputlist1[j][4]==str1):\n",
    "        count=count+1\n",
    "    return count\n",
    "\n",
    "print(\"\\n\\t\\t Naive Bayesian Classifier\")\n",
    "\n",
    "with open('data5.csv','r') as csv_file1:\n",
    "              csv_reader1=csv.reader(csv_file1)\n",
    "              for i in range(11):\n",
    "                  next(csv_reader1)\n",
    "              for line1 in csv_reader1:\n",
    "                  inputlist1.append(line1)\n",
    "              for j in range(1,len(inputlist1)):\n",
    "                  print(\"\\nThe \",j,\"Test data is:\\n\",head[0],\" = \",inputlist1[j][0],\", \",head[1],\" = \",inputlist1[j][1],\", \",head[2],\" = \",inputlist1[j][2],\", \",head[3],\" = \",inputlist1[j][3])\n",
    "\n",
    "                  #Declaring list variable for storing the result\n",
    "                  listyes=list()\n",
    "                  listno=list()\n",
    "                  resultyes=0.0\n",
    "                  resultno=0.0\n",
    "\n",
    "                  #Evaluating the Probability\n",
    "                  for d in range(4):\n",
    "                      listyes.append(df1.loc[df1[head[d]]==inputlist1[j][d],head[d]].count()/len(df1))\n",
    "                      listno.append(df2.loc[df2[head[d]]==inputlist1[j][d],head[d]].count()/len(df2))\n",
    "                  resultyes = np.prod(np.array(listyes))*(len(df1)/len(data1))\n",
    "                  resultno = np.prod(np.array(listno))*(len(df2)/len(data1))\n",
    "                  print(\"Probability of Yes: \",resultyes,\"\\nProbability of No: \",resultno)\n",
    "                  if resultyes>resultno:\n",
    "                     print(\"Classified as YES\\n\")\n",
    "                     count=counting(inputlist1,j,'Yes',count)\n",
    "                  else:\n",
    "                     print(\"Classified as NO\\n\")\n",
    "                     count=counting(inputlist1,j,'No',count)\n",
    "\n",
    "print(\"\\nAccuracy of the Classifier is: \",count/(len(inputlist1)-1) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total instances in the dataset:  18\n",
      "\n",
      " Dataset is Split into Training and Testing Samples\n",
      "\n",
      " Training Instances:  13\n",
      "10                 This is an awesome place\n",
      "12                          I love to dance\n",
      "6                  I am tired of this stuff\n",
      "1                  This is an amazing place\n",
      "11    I do not like the taste of this juice\n",
      "4                      What an awesome view\n",
      "9                       My boss is horrible\n",
      "14                     What a great holiday\n",
      "15           That is a bad locality to stay\n",
      "2       I feel very good about these places\n",
      "13        I am sick and tired of this place\n",
      "0                      I love this sandwich\n",
      "8                      He is my sworn enemy\n",
      "Name: message, dtype: object\n",
      "\n",
      " Testing Instances : 5\n",
      "16       We will have good fun tomorrow\n",
      "5         I do not like this restaurant\n",
      "7                I can't deal with this\n",
      "17    I went to my enemey's house today\n",
      "3                  This is my best work\n",
      "Name: message, dtype: object\n",
      "\n",
      " Total features extracted using CountVectorizer:  42\n",
      "\n",
      " Features for first 5 training instances are listed below\n",
      "   about  am  amazing  an  and  awesome  bad  boss  dance  do  ...  taste  \\\n",
      "0      0   0        0   1    0        1    0     0      0   0  ...      0   \n",
      "1      0   0        0   0    0        0    0     0      1   0  ...      0   \n",
      "2      0   1        0   0    0        0    0     0      0   0  ...      0   \n",
      "3      0   0        1   1    0        0    0     0      0   0  ...      0   \n",
      "4      0   0        0   0    0        0    0     0      0   1  ...      1   \n",
      "\n",
      "   that  the  these  this  tired  to  very  view  what  \n",
      "0     0    0      0     1      0   0     0     0     0  \n",
      "1     0    0      0     0      0   1     0     0     0  \n",
      "2     0    0      0     1      1   0     0     0     0  \n",
      "3     0    0      0     1      0   0     0     0     0  \n",
      "4     0    1      0     1      0   0     0     0     0  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Classification Results of Test Dataset are:\n",
      "\n",
      "We will have good fun tomorrow -->  pos \n",
      "I do not like this restaurant -->  neg \n",
      "I can't deal with this -->  pos \n",
      "I went to my enemey's house today -->  neg \n",
      "This is my best work -->  neg \n",
      "\n",
      "Accuracy of the classifier is 0.6\n",
      "\n",
      "Confusion Matrix\n",
      "[[2 1]\n",
      " [1 1]]\n",
      "\n",
      "Recall and Precision\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "msg=pd.read_csv('data6.csv',names=['message','label'])\n",
    "print('\\n Total instances in the dataset: ',msg.shape[0])\n",
    "msg['labelnum']=msg.label.map({'pos':1,'neg':0})\n",
    "x=msg.message\n",
    "y=msg.labelnum\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y)\n",
    "\n",
    "print('\\n Dataset is Split into Training and Testing Samples')\n",
    "print('\\n Training Instances: ',xtrain.shape[0])\n",
    "print(xtrain)\n",
    "print('\\n Testing Instances :',xtest.shape[0])\n",
    "print(xtest)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "xtrain_dtm = count_vect.fit_transform(xtrain)\n",
    "xtest_dtm = count_vect.transform(xtest)\n",
    "print('\\n Total features extracted using CountVectorizer: ',xtrain_dtm.shape[1])\n",
    "\n",
    "print('\\n Features for first 5 training instances are listed below')\n",
    "df = pd.DataFrame(xtrain_dtm.toarray(),columns=count_vect.get_feature_names())\n",
    "print(df[0:5])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB().fit(xtrain_dtm,ytrain)\n",
    "predicted=clf.predict(xtest_dtm)\n",
    "\n",
    "print('\\nClassification Results of Test Dataset are:\\n')\n",
    "for doc, p in zip(xtest,predicted):\n",
    "    pred = 'pos' if p==1 else 'neg'\n",
    "    print('%s -->  %s '%(doc,pred))\n",
    "\n",
    "from sklearn import metrics\n",
    "print('\\nAccuracy of the classifier is',metrics.accuracy_score(ytest,predicted))\n",
    "print('\\nConfusion Matrix')\n",
    "print(metrics.confusion_matrix(ytest,predicted))\n",
    "print('\\nRecall and Precision')\n",
    "print(metrics.recall_score(ytest,predicted))\n",
    "print(metrics.precision_score(ytest,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bayesian Network Nodes are: \n",
      "\t ['age', 'trestbps', 'fbs', 'sex', 'exang', 'heartdisease', 'restecg', 'thalach', 'chol']\n",
      "\n",
      "Bayesian Network Edges are:\n",
      "\t [('age', 'trestbps'), ('age', 'fbs'), ('trestbps', 'heartdisease'), ('fbs', 'heartdisease'), ('sex', 'trestbps'), ('exang', 'trestbps'), ('heartdisease', 'restecg'), ('heartdisease', 'thalach'), ('heartdisease', 'chol')]\n",
      "\n",
      "Learning CPDs using Maximum Likelihood Estimators...\n",
      "\n",
      "Inferencing with Bayesian Network:\n",
      "\n",
      "1.Probability of HeartDisease given Age=28\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "28",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-81c3823f1c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Computing the probability of bronc given smoke.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n1.Probability of HeartDisease given Age=28'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHeartDisease_infer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'heartdisease'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'heartdisease'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n2. Probability of HeartDisease given chol (Cholestoral) =100'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\inference\\ExactInference.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, variables, evidence, elimination_order, joint, show_progress)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0melimination_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melimination_order\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mjoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\inference\\ExactInference.py\u001b[0m in \u001b[0;36m_variable_elimination\u001b[1;34m(self, variables, operation, evidence, elimination_order, joint, show_progress)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0meliminated_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;31m# Get working factors and elimination order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mworking_factors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_working_factors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevidence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         elimination_order = self._get_elimination_order(\n\u001b[0;32m    161\u001b[0m             \u001b[0mvariables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melimination_order\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\inference\\ExactInference.py\u001b[0m in \u001b[0;36m_get_working_factors\u001b[1;34m(self, evidence)\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworking_factors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                     factor_reduced = factor.reduce(\n\u001b[1;32m---> 46\u001b[1;33m                         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevidence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevidence_var\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                     )\n\u001b[0;32m     48\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfactor_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, values, inplace)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         values = [\n\u001b[1;32m--> 441\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_no\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m         ]\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\factors\\discrete\\DiscreteFactor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         values = [\n\u001b[1;32m--> 441\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_no\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m         ]\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pgmpy\\utils\\state_name.py\u001b[0m in \u001b[0;36mget_state_no\u001b[1;34m(self, var, state_name)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \"\"\"\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_to_no\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstate_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 28"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "#Read the attributes\n",
    "lines = list(csv.reader(open('data7_names.csv', 'r')));\n",
    "attributes = lines[0]\n",
    "\n",
    "#Read Cleveland Heart dicease data\n",
    "heartDisease = pd.read_csv('data7_heart.csv', names = attributes)\n",
    "heartDisease = heartDisease.replace('?', np.nan)\n",
    "\n",
    "# Model Baysian Network\n",
    "model = BayesianModel([('age', 'trestbps'), ('age', 'fbs'), ('sex', 'trestbps'), ('sex', 'trestbps'),\n",
    "('exang', 'trestbps'),('trestbps','heartdisease'),('fbs','heartdisease'),\n",
    "('heartdisease','restecg'),('heartdisease','thalach'),('heartdisease','chol')])\n",
    "\n",
    "\n",
    "print('\\nBayesian Network Nodes are: ')\n",
    "print('\\t',model.nodes())\n",
    "print('\\nBayesian Network Edges are:')\n",
    "print('\\t',model.edges())\n",
    "\n",
    "\n",
    "# Learning CPDs using Maximum Likelihood Estimators\n",
    "print('\\nLearning CPDs using Maximum Likelihood Estimators...');\n",
    "model.fit(heartDisease, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "# Inferencing with Bayesian Network\n",
    "print('\\nInferencing with Bayesian Network:')\n",
    "HeartDisease_infer = VariableElimination(model)\n",
    "\n",
    "# Computing the probability of bronc given smoke.\n",
    "print('\\n1.Probability of HeartDisease given Age=28')\n",
    "q = HeartDisease_infer.query(variables=['heartdisease'], evidence={'age': 37},joint=False)\n",
    "print(q['heartdisease'])\n",
    "print('\\n2. Probability of HeartDisease given chol (Cholestoral) =100')\n",
    "q = HeartDisease_infer.query(variables=['heartdisease'], evidence={'chol': 100},)\n",
    "print(q['heartdisease'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = pd.DataFrame(iris.data)\n",
    "X.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']\n",
    "y = pd.DataFrame(iris.target)\n",
    "y.columns = ['Targets']\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X)\n",
    "\n",
    "plt.figure(figsize=(14,14))\n",
    "colormap = np.array(['red', 'lime', 'black'])\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)\n",
    "plt.title('Real Clusters')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[model.labels_], s=40)\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "xsa = scaler.transform(X)\n",
    "xs = pd.DataFrame(xsa, columns = X.columns)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(xs)\n",
    "gmm_y = gmm.predict(xs)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[gmm_y], s=40)\n",
    "plt.title('GMM Clustering')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.show()\n",
    "\n",
    "print('Observation: The GMM using EM algorithm based clustering matched the')\n",
    "print(' true labels more closely than the Kmeans.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "iris_data=iris.data\n",
    "iris_labels=iris.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(iris_data,iris_labels,test_size=0.20)\n",
    "\n",
    "classifier=KNeighborsClassifier(5)\n",
    "classifier.fit(x_train,y_train)\n",
    "\n",
    "y_pred=classifier.predict(x_test)\n",
    "\n",
    "print('Confusion Matrix is as follows')\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('Accuracy Metrics')\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import operator\n",
    "from os import listdir\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy.linalg\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "def kernel(point,xmat,k):\n",
    "    m,n= shape(xmat)\n",
    "    weights=mat(eye((m)))\n",
    "    for j in range(m):\n",
    "        diff = point - X[j]\n",
    "        weights[j,j]= exp(diff*diff.T/(-2*k**2))\n",
    "    return weights\n",
    "\n",
    "def localWeight(point,xmat,ymat,k):\n",
    "    wei=kernel(point,xmat,k)\n",
    "    W=(X.T*(wei*X)).I*(X.T*(wei*ymat.T))\n",
    "    return W\n",
    "\n",
    "def localWeightRegression(xmat,ymat,k):\n",
    "    m,n=shape(xmat)\n",
    "    ypred=zeros(m)\n",
    "    for i in range(m):\n",
    "        ypred[i]=xmat[i]*localWeight(xmat[i],xmat,ymat,k)\n",
    "    return ypred\n",
    "\n",
    "#load data points\n",
    "data=pd.read_csv('data10.csv')\n",
    "bill=array(data.totbill)\n",
    "tip=array(data.tip)\n",
    "\n",
    "#Preparing and add 1 in bill\n",
    "mbill=mat(bill)\n",
    "mtip=mat(tip)\n",
    "m=shape(mbill)[1]\n",
    "one=mat(ones(m))\n",
    "X=hstack((one.T,mbill.T))\n",
    "\n",
    "#set k here\n",
    "ypred=localWeightRegression(X,mtip,0.5)\n",
    "SortIndex=X[:,1].argsort(0)\n",
    "xsort=X[SortIndex][:,0]\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(1,1,1)\n",
    "ax.scatter(bill,tip,color='green')\n",
    "ax.plot(xsort[:,1],ypred[SortIndex],color='red',linewidth=5)\n",
    "plt.xlabel('Total Bill')\n",
    "plt.ylabel('Tip')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
